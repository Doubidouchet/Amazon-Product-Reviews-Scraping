{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import plotly.express as px\n",
    "from selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pour générer l'URL en fonction du terme recherché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_by_term(search_term,num_page):\n",
    "    form = \"https://www.amazon.com/s?k={}&ref=nb_sb_noss_1&page=\"+str(num_page)\n",
    "    term = search_term.replace(\" \", \"+\")\n",
    "    return form.format(term)\n",
    "\n",
    "term = \"portable\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des liens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici j'extraie les liens de redirections\n",
    "links = []\n",
    "nb_pages_searching_result = 60\n",
    "for i in range(nb_pages_searching_result):\n",
    "    \n",
    "    url = url_by_term(term,i)\n",
    "    driver.get(url)\n",
    "\n",
    "    bs = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    results = bs.find_all('h2', {'class':'a-size-mini a-spacing-none a-color-base s-line-clamp-4'})\n",
    "    \n",
    "    if len(results)==0:\n",
    "        results = bs.find_all('h2', {'class':'a-size-mini a-spacing-none a-color-base s-line-clamp-2'})\n",
    "\n",
    "    for r in results:\n",
    "        \n",
    "        links.append('https://www.amazon.com'+r.find('a')['href'])\n",
    "\n",
    "#Ici j'extraie les vrais lien à partir des liens de redirections\n",
    "c=0\n",
    "newlink = []\n",
    "for x in links: \n",
    "    \n",
    "    driver.get(x)\n",
    "    newlink.append(driver.current_url)\n",
    "    c = c+1\n",
    "    print(c)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction d'avis et de commentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in range(len(newlink)):\n",
    "    newlink[j] = newlink[j].replace(\"dp\",\"product-reviews\")\n",
    "\n",
    "nb_pages_review = 20\n",
    "review = []    \n",
    "list_links=[]\n",
    "positif=[]\n",
    "c1=0\n",
    "for g in newlink:\n",
    "    c1 = c1+1\n",
    "    print(c1)\n",
    "    for i in range(nb_pages_review):\n",
    "        for j in ['positive','critical']:\n",
    "            list_links.append(g+\"&filterByStar=\"+j+\"&pageNumber=\"+str(i))\n",
    "            #driver.get(g+\"&filterByStar=\"+j+\"&pageNumber=\"+str(i))\n",
    "            driver.get(g+\"&filterByStar=\"+j+\"&pageNumber=\"+str(i))\n",
    "\n",
    "            bs = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            results = bs.find_all('span', {'data-hook':'review-body'})\n",
    "\n",
    "           \n",
    "            for r in results:\n",
    "                review.append(r.find('span').text)\n",
    "\n",
    "\n",
    "                if j == 'positive':\n",
    "                    positif.append(1)\n",
    "                if j == 'critical':\n",
    "                    positif.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raccourcis des commentaires (Deux premières et deux dernières phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "for i in review:\n",
    "    \n",
    "    liste= re.split('[?.!;]', i)\n",
    "    \n",
    "    if len(liste)>4:\n",
    "        j = liste[0]+\".\"+liste[1]+\".\"+liste[-3]+\".\"+liste[-2]+\".\"\n",
    "    if len(liste)==4:\n",
    "        j = liste[0]+\".\"+liste[1]+\".\"+liste[2]+\".\"\n",
    "    if len(liste)==3:\n",
    "        j = liste[0]+\".\"+liste[1]+\".\"\n",
    "    if len(liste)==2:\n",
    "        j = liste[0]+\".\"\n",
    "    if len(liste)==1:\n",
    "        j = liste[0]+\".\"\n",
    "        \n",
    "    review_list.append(j)\n",
    "    \n",
    "review_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage et Mise en Format JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = {'review' : review_list, 'positive': positif}\n",
    "review_df = pd.DataFrame(data=d)\n",
    "review_df = review_df.replace('\\n','', regex=True)\n",
    "review_df = review_df.replace('\\u2019','`', regex=True)\n",
    "review_df = review_df.replace('\\u201d','', regex=True)\n",
    "review_df = review_df.replace('\\u201c','', regex=True)\n",
    "review_df = review_df.replace('\\ud83d','', regex=True)\n",
    "review_df = review_df.replace('\\udc22','', regex=True)\n",
    "review_df = review_df.replace('\\/',' ', regex=True)\n",
    "\n",
    "review_df.drop_duplicates(keep = 'first', inplace=True)\n",
    "\n",
    "review_df.to_json(r'C:\\Users\\cleme\\OneDrive\\Documents\\Python Scripts\\Amazon Web Scrapping\\review_amazon_'+term.replace(' ','')+str(time.strftime(\"%d_%B_%Y\"))+'.json',orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
